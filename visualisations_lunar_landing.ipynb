{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves for different gamma values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Callback to log rewards\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(RewardLoggerCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if 'rewards' in self.locals:\n",
    "            reward = self.locals['rewards'][0]\n",
    "            if len(self.episode_rewards) == 0 or self.locals['dones'][0]:\n",
    "                self.episode_rewards.append(0)\n",
    "            self.episode_rewards[-1] += reward\n",
    "        return True\n",
    "\n",
    "def train_and_log_rewards(gamma, total_timesteps=250000): # can alter timesteps\n",
    "    \"\"\"\n",
    "    Trains a DQN agent with a specified discount factor and logs rewards.\n",
    "    Args:\n",
    "    \"\"\"\n",
    "    env = gym.make('LunarLander-v3')\n",
    "\n",
    "    # Initialise the reward logger\n",
    "    reward_logger = RewardLoggerCallback()\n",
    "\n",
    "    # Create DQN model with the specified gamma\n",
    "    gamma_model = DQN(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=0.00063,         # Optimal learning rate for Lunar Lander\n",
    "        gamma=gamma,                  # Discount factor\n",
    "        buffer_size=100000,\n",
    "        learning_starts=1000,\n",
    "        train_freq=4,\n",
    "        batch_size=128,\n",
    "        policy_kwargs=dict(net_arch=[256, 256]),\n",
    "        target_update_interval=250,\n",
    "        exploration_fraction=0.12,\n",
    "        exploration_final_eps=0.1,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    gamma_model.learn(total_timesteps=total_timesteps, callback=reward_logger)\n",
    "    return reward_logger.episode_rewards\n",
    "\n",
    "def plot_gamma_learning_curves(all_rewards, gamma_values, window=20):\n",
    "  \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for rewards, gamma in zip(all_rewards, gamma_values):\n",
    "        # Smooth the rewards using a moving average\n",
    "        smoothed_rewards = [np.mean(rewards[i:i+window]) for i in range(len(rewards) - window)]\n",
    "        plt.plot(smoothed_rewards, label=f\"Gamma = {gamma}\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Smoothed Reward\")\n",
    "    plt.title(\"Learning Curves for Different Gamma Values\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Define the gamma values to compare\n",
    "gamma_values = [0.99, 0.9]\n",
    "\n",
    "# Train the model for each gamma value and log rewards\n",
    "all_rewards = []\n",
    "for gamma in gamma_values:\n",
    "    print(f\"Training with gamma: {gamma}\")\n",
    "    rewards = train_and_log_rewards(gamma=gamma, total_timesteps=250000)\n",
    "    all_rewards.append(rewards)\n",
    "\n",
    "# Plot the learning curves\n",
    "plot_gamma_learning_curves(all_rewards, gamma_values, window=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves for different learning rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_rewards(learning_rate, total_timesteps=100000):\n",
    "    \"\"\"\n",
    "    Trains a DQN agent with a specified learning rate and logs rewards.\n",
    "    Args:\n",
    "    \"\"\"\n",
    "    env = gym.make('LunarLander-v3')\n",
    "\n",
    "    # Initialise the reward logger\n",
    "    reward_logger = RewardLoggerCallback()\n",
    "\n",
    "    # Create DQN model with the specified learning rate\n",
    "    learning_rate_model = DQN(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=learning_rate,\n",
    "        gamma=0.99, # optimal discount factor\n",
    "        buffer_size=100000,\n",
    "        learning_starts=1000,\n",
    "        train_freq=4,\n",
    "        batch_size=128,\n",
    "        policy_kwargs=dict(net_arch=[256, 256]),\n",
    "        target_update_interval=250,\n",
    "        exploration_fraction=0.12,\n",
    "        exploration_final_eps=0.1,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    learning_rate_model.learn(total_timesteps=total_timesteps, callback=reward_logger)\n",
    "    return reward_logger.episode_rewards\n",
    "\n",
    "def plot_learning_curves(all_rewards, learning_rates, window=20):\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for rewards, lr in zip(all_rewards, learning_rates):\n",
    "        # Smooth the rewards using a moving average\n",
    "        smoothed_rewards = [np.mean(rewards[i:i+window]) for i in range(len(rewards) - window)]\n",
    "        plt.plot(smoothed_rewards, label=f\"Learning Rate = {lr}\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Smoothed Reward\")\n",
    "    plt.title(\"Learning Curves for Different Learning Rates\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Define the learning rates to compare\n",
    "learning_rates = [0.000002, 0.00063, 0.003]\n",
    "\n",
    "# Train the model for each learning rate and log rewards\n",
    "all_rewards = []\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with learning rate: {lr}\")\n",
    "    rewards = train_and_log_rewards(learning_rate=lr, total_timesteps=500000) # change timesteps here\n",
    "    all_rewards.append(rewards)\n",
    "\n",
    "# Plot the learning curves\n",
    "plot_learning_curves(all_rewards, learning_rates, window=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve using optimised parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model over large number of timesteps eg. 1000000\n",
    "env = gym.make('LunarLander-v3')\n",
    "\n",
    "# Initialise DQN model\n",
    "trained_model = DQN(\n",
    "    policy=\"MlpPolicy\",          \n",
    "    env=env,                     \n",
    "    learning_rate=0.00063,      \n",
    "    gamma=0.99,                  \n",
    "    buffer_size=100000,         \n",
    "    learning_starts=1000,        \n",
    "    train_freq=4,                \n",
    "    batch_size=128,              \n",
    "    policy_kwargs=dict(net_arch=[256, 256]),                            \n",
    "    target_update_interval=250,  \n",
    "    exploration_fraction=0.12,    \n",
    "    exploration_final_eps=0.1,  \n",
    "    verbose=1                   \n",
    ")\n",
    "\n",
    "# track rewards \n",
    "reward_logger = RewardLoggerCallback()\n",
    "\n",
    "trained_model.learn(total_timesteps=1000000)\n",
    "\n",
    "# save trained model\n",
    "trained_model.save(\"dict_lunar_lander_dqn\")\n",
    "\n",
    "def plot_learning_curve(rewards, window=20):\n",
    "\n",
    "    # Compute moving average to smooth curve\n",
    "    moving_avg = [np.mean(rewards[i-window:i]) for i in range(window, len(rewards))]\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(window, len(rewards)), moving_avg)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Learning Curve for DQN')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curve(reward_logger.episode_rewards, window=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of rewards of an untrained and trained DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_agent(env, model, n_episodes=100):\n",
    "    \n",
    "    rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Take random actions since the agent is untrained\n",
    "            action = env.action_space.sample()  # Random action\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Create the Lunar Lander environment\n",
    "env = gym.make('LunarLander-v3')\n",
    "\n",
    "# Create an untrained DQN model\n",
    "untrained_model = DQN(policy=\"MlpPolicy\", env=env, verbose=0)  # Do NOT train the model\n",
    "\n",
    "# save model\n",
    "untrained_model.save(\"lunar_lander_dqn\")\n",
    "\n",
    "# Evaluate the untrained agent\n",
    "untrained_rewards = evaluate_agent(env, untrained_model, n_episodes=100)\n",
    "\n",
    "# Plot the histogram of rewards for the untrained agent\n",
    "plt.hist(untrained_rewards, bins=10, edgecolor='k')\n",
    "plt.xlabel(\"Episode Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Episode Rewards - Untrained Agent\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of rewards for the untrained agent\n",
    "trained_rewards = evaluate_agent(env, trained_model, n_episodes=100)\n",
    "\n",
    "plt.hist(untrained_rewards, bins=10, edgecolor='k')\n",
    "plt.xlabel(\"Episode Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Episode Rewards - Trained Agent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode video sample from untrained and trained agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record untrained agent\n",
    "def record_untrained_agent(video_path, n_episodes=1):\n",
    "    \n",
    "    # Create the environment and wrap it with the RecordVideo wrapper\n",
    "    env = gym.make('LunarLander-v3', render_mode=\"rgb_array\")\n",
    "    env = gym.wrappers.RecordVideo(env, video_path, episode_trigger=lambda episode_id: True)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Random action for untrained agent\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "    env.close()\n",
    "\n",
    "# Record the untrained agent video\n",
    "record_untrained_agent(video_path=\"./untrained_agent_video\", n_episodes=1)\n",
    "\n",
    "# video of agent in trained eviroment\n",
    "\n",
    "def record_trained_agent(model_path, video_path, n_episodes=1):\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = DQN.load(model_path)\n",
    "\n",
    "    # Create the environment and wrap it with the RecordVideo wrapper\n",
    "    env = gym.make('LunarLander-v3', render_mode=\"rgb_array\")\n",
    "    env = gym.wrappers.RecordVideo(env, video_path, episode_trigger=lambda episode_id: True)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Predict the next action using the trained model\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "    env.close()\n",
    "\n",
    "# Record the trained agent video\n",
    "record_trained_agent(model_path=\"dict_lunar_lander_dqn\", video_path=\"./trained_agent_video\", n_episodes=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgement:\n",
    "I acknowledge the use of ChatGPT-4o (OpenAI,\n",
    "https://chat.openai.com/) to assist in debugging functions to generate working plots. I confirm that no content generated by AI has been presented as my own work.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cw_two",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
